{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6779,"status":"ok","timestamp":1752037962352,"user":{"displayName":"J. Nevil","userId":"11485525677654936583"},"user_tz":-330},"id":"fmdK3zesK2WI","outputId":"92b5d45a-4788-4d27-a200-c7c7457dff91"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ustlb29z\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ustlb29z\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"]}],"source":["!pip install git+https://github.com/openai/CLIP.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FH59UQHILEsC"},"outputs":[],"source":["import os\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","class HistopathologyDataset(Dataset):\n","    def __init__(self, image_dir, label_dict, preprocess):\n","        self.image_dir = image_dir\n","        self.image_files = list(label_dict.keys())\n","        self.labels = list(label_dict.values())\n","        self.preprocess = preprocess\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.image_dir, self.image_files[idx])\n","        image = self.preprocess(Image.open(img_path).convert(\"RGB\"))\n","        label = self.labels[idx]\n","        return image, label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":42347,"status":"ok","timestamp":1752037861348,"user":{"displayName":"J. Nevil","userId":"11485525677654936583"},"user_tz":-330},"id":"E0F0NGj4LJJv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"41ab5271-25d9-45d5-d7dc-3110f70a5c82"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 244M/244M [00:37<00:00, 6.76MiB/s]\n"]}],"source":["import torch\n","import torch.nn as nn\n","import clip\n","\n","# ‚úÖ Set device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# ‚úÖ Load CLIP ResNet-50 model\n","clip_model, preprocess = clip.load(\"RN50\", device=device)\n","\n","# ‚úÖ Define classifier with correct input dimension (1024 for RN50)\n","class CLIPClassifier(nn.Module):\n","    def __init__(self, clip_model):\n","        super().__init__()\n","        self.image_encoder = clip_model.visual\n","        self.classifier = nn.Linear(1024, 2)  # 2 classes: e.g., benign vs malignant\n","\n","    def forward(self, images):\n","        with torch.no_grad():  # freeze encoder\n","            features = self.image_encoder(images)\n","        return self.classifier(features)\n","\n","# ‚úÖ Example: Initialize model\n","model = CLIPClassifier(clip_model).to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrgRFwdCLRap"},"outputs":[],"source":["import os\n","\n","def create_label_dict(base_dir):\n","    label_dict = {}\n","    class_to_idx = {}\n","    current_label = 0\n","\n","    # Traverse benign and malignant directories\n","    for category in [\"benign\", \"malignant\"]:\n","        category_path = os.path.join(base_dir, category)\n","        for subfolder in os.listdir(category_path):\n","            class_path = os.path.join(category_path, subfolder)\n","            if os.path.isdir(class_path):\n","                class_to_idx[subfolder] = current_label\n","                for file in os.listdir(class_path):\n","                    if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","                        label_dict[file] = current_label\n","                current_label += 1\n","\n","    return label_dict, class_to_idx\n","\n","# Set your paths\n","train_path = \"/content/drive/MyDrive/Colab Notebooks/sample/TRAIN\"\n","valid_path = \"/content/drive/MyDrive/Colab Notebooks/sample/VALID\"\n","\n","train_labels, train_classes = create_label_dict(train_path)\n","val_labels, val_classes = create_label_dict(valid_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ii8ns82zMzLM"},"outputs":[],"source":["from torch.utils.data import Dataset\n","from PIL import Image\n","import os\n","\n","class HistopathologyDataset(Dataset):\n","    def __init__(self, image_dir, label_dict, preprocess):\n","        self.image_dir = image_dir\n","        self.label_dict = label_dict\n","        self.preprocess = preprocess\n","        self.image_files = list(label_dict.keys())\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_files[idx]\n","        label = self.label_dict[img_name]\n","\n","        # Search both benign and malignant subdirs\n","        for category in [\"benign\", \"malignant\"]:\n","            for subfolder in os.listdir(os.path.join(self.image_dir, category)):\n","                image_path = os.path.join(self.image_dir, category, subfolder, img_name)\n","                if os.path.exists(image_path):\n","                    image = Image.open(image_path).convert(\"RGB\")\n","                    image = self.preprocess(image)\n","                    return image, label\n","\n","        raise FileNotFoundError(f\"{img_name} not found in {self.image_dir}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tWKbAQUGNE-n","outputId":"723c0c87-d784-476b-dda4-29d1a2239fbe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: open-clip-torch in /usr/local/lib/python3.11/dist-packages (2.32.0)\n","Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.21.0+cu124)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2024.11.6)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (6.3.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (4.67.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.33.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.5.3)\n","Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (1.0.16)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->open-clip-torch) (1.3.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch) (0.2.13)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (2.32.3)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (1.1.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.6.15)\n"]}],"source":["!pip install open-clip-torch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1_dpkGCW5ZY5aWTo1hgDiXnrKK2GmU4IL"},"executionInfo":{"elapsed":68197,"status":"ok","timestamp":1751879056518,"user":{"displayName":"J. Nevil","userId":"11485525677654936583"},"user_tz":-330},"id":"75P5U7m0lqKw","outputId":"070725ac-d1b2-49b2-851d-4a74f561dffe"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# ===== TRAIN Folder =====\n","train_path = '/content/drive/MyDrive/Colab Notebooks/sample/TRAIN'\n","\n","print(\"üìÇ Scanning TRAIN folder...\")\n","for root, dirs, files in os.walk(train_path):\n","    for fname in files:\n","        if fname.lower().endswith('.png'):\n","            try:\n","                img_path = os.path.join(root, fname)\n","                img = Image.open(img_path)\n","\n","                plt.imshow(img)\n","                plt.title(f\"TRAIN - {fname}\")\n","                plt.axis('off')\n","                plt.show()\n","\n","            except Exception as e:\n","                print(f\"‚ùå Error loading TRAIN image {img_path}: {e}\")\n","\n","# ===== VALID Folder =====\n","valid_path = '/content/drive/MyDrive/Colab Notebooks/sample/VALID'\n","\n","print(\"\\nüìÇ Scanning VALID folder...\")\n","for root, dirs, files in os.walk(valid_path):\n","    for fname in files:\n","        if fname.lower().endswith('.png'):\n","            try:\n","                img_path = os.path.join(root, fname)\n","                img = Image.open(img_path)\n","\n","                plt.imshow(img)\n","                plt.title(f\"VALID - {fname}\")\n","                plt.axis('off')\n","                plt.show()\n","\n","            except Exception as e:\n","                print(f\"‚ùå Error loading VALID image {img_path}: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XROdGXUB9z1v"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WqPHFWNdNI--"},"outputs":[],"source":["import open_clip\n","import torch\n","import torch.nn as nn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["901555769a7b45f9964a3322c709f462","d331c4417395438fa6b7d26c711e1bd8","15028bbd37ef45bcb2cf9e465ca96d83","a6c372c358534887b4edbbc0c956711a","c6c7a4f78deb4b4fae029b1aa9d0105a","5e6c59004aa9483e90298af17b88697f","7b596e26bdcf470dbb39b488999763f7","f765a1cec627421cb43f5adc7395f3c3","f562f9bc980f4bb8b56127082495eba8","02ff6904a77642e6ac8b16a556364dbf","08d17d39f5ed4c319482b6fe5f80d6bb"]},"executionInfo":{"elapsed":16727,"status":"ok","timestamp":1751879073839,"user":{"displayName":"J. Nevil","userId":"11485525677654936583"},"user_tz":-330},"id":"Um3yxn8PNMrd","outputId":"7867bbf0-6b2d-45dc-e2d0-4b4a06510761"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n","Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n","You are not authenticated with the Hugging Face Hub in this notebook.\n","If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"901555769a7b45f9964a3322c709f462","version_major":2,"version_minor":0},"text/plain":["open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Load pre-trained CLIP\n","model, _, preprocess_clip = open_clip.create_model_and_transforms(\n","    'ViT-B-32',\n","    pretrained='laion2b_s34b_b79k',\n","    device=device\n",")\n","\n","# Use only the visual encoder\n","image_encoder = model.visual\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ItLwdTydNV8b"},"outputs":[],"source":["# Freeze CLIP encoder\n","for param in image_encoder.parameters():\n","    param.requires_grad = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"E-nc22nLNXnE"},"outputs":[],"source":["class CLIPClassifier(nn.Module):\n","    def __init__(self, encoder, num_classes):\n","        super(CLIPClassifier, self).__init__()\n","        self.encoder = encoder\n","        self.classifier = nn.Sequential(\n","            nn.LayerNorm(512),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)            # Output: [B, 512]\n","        x = self.classifier(x)         # Output: [B, num_classes]\n","        return x\n","\n","num_classes = len(train_classes)  # 8 for your case (4 benign + 4 malignant)\n","model = CLIPClassifier(image_encoder, num_classes).to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dDYGB5doNbnc"},"outputs":[],"source":["from torch import optim\n","import torch.nn.functional as F\n","\n","optimizer = optim.AdamW(model.classifier.parameters(), lr=1e-4)\n","\n","def train_one_epoch(model, dataloader):\n","    model.train()\n","    total_loss = 0\n","    for images, labels in dataloader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        loss = F.cross_entropy(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    return total_loss / len(dataloader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DFphrcprNgws"},"outputs":[],"source":["def evaluate(model, dataloader):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            predictions = torch.argmax(outputs, dim=1)\n","            correct += (predictions == labels).sum().item()\n","            total += labels.size(0)\n","    return correct / total\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sjW9zeOSNjXE"},"outputs":[],"source":["def evaluate(model, dataloader):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            predictions = torch.argmax(outputs, dim=1)\n","            correct += (predictions == labels).sum().item()\n","            total += labels.size(0)\n","    return correct / total\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"suki-FbpNnRY"},"outputs":[],"source":["def evaluate(model, dataloader):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            predictions = torch.argmax(outputs, dim=1)\n","            correct += (predictions == labels).sum().item()\n","            total += labels.size(0)\n","    return correct / total\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Okj3i8nYniw9"},"outputs":[],"source":["from torch.utils.data import Dataset\n","import os\n","from PIL import Image\n","\n","class HistologyCLIPDataset(Dataset):\n","    def __init__(self, root_dir, preprocess, classnames=None):\n","        self.root_dir = root_dir\n","        self.preprocess = preprocess  # CLIP image transform\n","        self.image_paths = []\n","        self.labels = []\n","        self.classnames = sorted(os.listdir(root_dir)) if classnames is None else classnames\n","\n","        for label, classname in enumerate(self.classnames):\n","            class_dir = os.path.join(root_dir, classname)\n","            for fname in os.listdir(class_dir):\n","                if fname.lower().endswith('.png'):\n","                    self.image_paths.append(os.path.join(class_dir, fname))\n","                    self.labels.append(label)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        label = self.labels[idx]\n","        image = Image.open(img_path).convert(\"RGB\")\n","        image = self.preprocess(image)  # Resize, normalize\n","        return image, label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1751879073853,"user":{"displayName":"J. Nevil","userId":"11485525677654936583"},"user_tz":-330},"id":"T8RQxq7voMjK","outputId":"631b90f2-dd0f-4e50-a8aa-a5a1c96018bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset size: 3\n","Valid dataset size: 3\n"]}],"source":["import os\n","\n","train_path = \"/content/drive/MyDrive/Colab Notebooks/sample/TRAIN\"\n","valid_path = \"/content/drive/MyDrive/Colab Notebooks/sample/VALID\"\n","\n","print(f\"Train dataset size: {len(os.listdir(train_path))}\")\n","print(f\"Valid dataset size: {len(os.listdir(valid_path))}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20613,"status":"ok","timestamp":1751879094462,"user":{"displayName":"J. Nevil","userId":"11485525677654936583"},"user_tz":-330},"id":"lUrZmLnQonym","outputId":"aba89958-07ce-47e4-81e3-237403dfc324"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-yf3djdk7\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-yf3djdk7\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"]}],"source":["!pip install torch torchvision ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amcUuuZupfXE"},"outputs":[],"source":["import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import clip\n","\n","class HistologyCLIPDataset(Dataset):\n","    def __init__(self, root_dir, preprocess, classnames):\n","        self.root_dir = root_dir\n","        self.preprocess = preprocess\n","        self.classnames = classnames\n","        self.image_paths = []\n","        self.labels = []\n","\n","        for label, class_name in enumerate(classnames):\n","            sub_path = os.path.join(root_dir, class_name)\n","            for img_file in os.listdir(sub_path):\n","                if img_file.endswith((\".png\", \".jpg\", \".jpeg\")):\n","                    self.image_paths.append(os.path.join(sub_path, img_file))\n","                    self.labels.append(label)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = self.preprocess(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n","        label = self.labels[idx]\n","        return image, label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fb0W_vGCphfr"},"outputs":[],"source":["import torch\n","import clip\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOqp20KipmP5"},"outputs":[],"source":["train_root = \"/content/drive/MyDrive/Colab Notebooks/sample/TRAIN\"\n","classnames = []\n","\n","# Get subfolder names recursively (2 levels: benign/adenosis)\n","for class_folder in sorted(os.listdir(train_root)):\n","    path = os.path.join(train_root, class_folder)\n","    if os.path.isdir(path):\n","        for subfolder in sorted(os.listdir(path)):\n","            classnames.append(f\"{class_folder}/{subfolder}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":6,"status":"error","timestamp":1751879101387,"user":{"displayName":"J. Nevil","userId":"11485525677654936583"},"user_tz":-330},"id":"O3sHNWMNpoGL","outputId":"e8541c87-9637-42d5-dba9-9fc87851306c"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/sample/VALID/benign/Fibroadenoma'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-22-4150153399.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHistologyCLIPDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHistologyCLIPDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/sample/VALID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-19-2185770291.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, preprocess, classnames)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0msub_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimg_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".jpeg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/sample/VALID/benign/Fibroadenoma'"]}],"source":["train_dataset = HistologyCLIPDataset(train_root, preprocess, classnames)\n","valid_dataset = HistologyCLIPDataset(\"/content/drive/MyDrive/Colab Notebooks/sample/VALID\", preprocess, classnames)\n","\n","from torch.utils.data import DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kayPA35gptWY"},"outputs":[],"source":["import torch.nn as nn\n","\n","# Freeze CLIP\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# Replace final head with custom classifier\n","class CLIPCustomClassifier(nn.Module):\n","    def __init__(self, clip_model, num_classes):\n","        super(CLIPCustomClassifier, self).__init__()\n","        self.clip = clip_model.visual\n","        self.classifier = nn.Linear(self.clip.output_dim, num_classes)\n","\n","    def forward(self, x):\n","        with torch.no_grad():\n","            features = self.clip(x)\n","        return self.classifier(features)\n","\n","model_custom = CLIPCustomClassifier(model, num_classes=len(classnames)).to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZJxfwjfpxx4"},"outputs":[],"source":["import torch.nn as nn\n","\n","# Freeze CLIP\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# Replace final head with custom classifier\n","class CLIPCustomClassifier(nn.Module):\n","    def __init__(self, clip_model, num_classes):\n","        super(CLIPCustomClassifier, self).__init__()\n","        self.clip = clip_model.visual\n","        self.classifier = nn.Linear(self.clip.output_dim, num_classes)\n","\n","    def forward(self, x):\n","        with torch.no_grad():\n","            features = self.clip(x)\n","        return self.classifier(features)\n","\n","model_custom = CLIPCustomClassifier(model, num_classes=len(classnames)).to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L9Zzq6OSp3iX"},"outputs":[],"source":["import torch.optim as optim\n","import torch.nn.functional as F\n","\n","optimizer = optim.Adam(model_custom.classifier.parameters(), lr=1e-4)\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model_custom.train()\n","    total_loss = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        logits = model_custom(images)\n","        loss = F.cross_entropy(logits, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnPVvA_osDQ_"},"outputs":[],"source":["model_custom.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for images, labels in valid_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model_custom(images)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-dTfTAKsT92"},"outputs":[],"source":["import os\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import clip\n","\n","# ----------------------------\n","# CONFIG\n","# ----------------------------\n","train_dir = \"/content/drive/MyDrive/Colab Notebooks/sample/TRAIN\"\n","valid_dir = \"/content/drive/MyDrive/Colab Notebooks/sample/VALID\"\n","batch_size = 4\n","num_epochs = 25\n","learning_rate = 1e-4\n","model_save_path = \"best_model.pt\"\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# ----------------------------\n","# CLASS NAMES: nested folders\n","# ----------------------------\n","classnames = []\n","for top in sorted(os.listdir(train_dir)):\n","    top_path = os.path.join(train_dir, top)\n","    if os.path.isdir(top_path):\n","        for sub in sorted(os.listdir(top_path)):\n","            if os.path.isdir(os.path.join(top_path, sub)):\n","                classnames.append(f\"{top}/{sub}\")\n","\n","# ----------------------------\n","# Dataset class\n","# ----------------------------\n","class HistologyCLIPDataset(Dataset):\n","    def __init__(self, root_dir, preprocess, classnames):\n","        self.root_dir = root_dir\n","        self.preprocess = preprocess\n","        self.classnames = classnames\n","        self.image_paths = []\n","        self.labels = []\n","\n","        for label, class_name in enumerate(classnames):\n","            folder = os.path.join(root_dir, class_name)\n","            if os.path.isdir(folder):\n","                for file in os.listdir(folder):\n","                    if file.endswith(('.png', '.jpg', '.jpeg')):\n","                        self.image_paths.append(os.path.join(folder, file))\n","                        self.labels.append(label)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = self.preprocess(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n","        label = self.labels[idx]\n","        return image, label\n","\n","# ----------------------------\n","# Load CLIP model\n","# ----------------------------\n","model_clip, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","# Freeze CLIP\n","for param in model_clip.parameters():\n","    param.requires_grad = False\n","\n","# ----------------------------\n","# Custom classifier model\n","# ----------------------------\n","class CLIPCustomClassifier(nn.Module):\n","    def __init__(self, clip_model, num_classes):\n","        super(CLIPCustomClassifier, self).__init__()\n","        self.clip = clip_model.visual\n","        self.classifier = nn.Linear(self.clip.output_dim, num_classes)\n","\n","    def forward(self, x):\n","        with torch.no_grad():\n","            features = self.clip(x)\n","        return self.classifier(features)\n","\n","model = CLIPCustomClassifier(model_clip, len(classnames)).to(device)\n","\n","# ----------------------------\n","# Dataset & DataLoaders\n","# ----------------------------\n","train_dataset = HistologyCLIPDataset(train_dir, preprocess, classnames)\n","valid_dataset = HistologyCLIPDataset(valid_dir, preprocess, classnames)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","\n","# ----------------------------\n","# Optimizer & Training Setup\n","# ----------------------------\n","optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n","best_val_acc = 0\n","patience = 3\n","wait = 0\n","\n","# ----------------------------\n","# Training Loop\n","# ----------------------------\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        loss = F.cross_entropy(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    print(f\"\\nEpoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f}\")\n","\n","    # ----------------------------\n","    # Validation\n","    # ----------------------------\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in valid_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (preds == labels).sum().item()\n","\n","    val_acc = 100 * correct / total\n","    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n","\n","    # Save best model\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        wait = 0\n","        torch.save(model.state_dict(), model_save_path)\n","        print(f\"‚úÖ Saved best model (Accuracy: {val_acc:.2f}%)\")\n","    else:\n","        wait += 1\n","        if wait >= patience:\n","            print(\"‚èπÔ∏è Early stopping triggered.\")\n","            break\n","\n","print(\"Training completed. Best validation accuracy:\", best_val_acc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRVpZdWauCHM"},"outputs":[],"source":["model.load_state_dict(torch.load(\"best_model.pt\"))\n","model.eval()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5OMp_IJueyQ"},"outputs":[],"source":["model.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for images, labels in valid_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, preds = torch.max(outputs, 1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","val_acc = 100 * correct / total\n","print(f\"Validation Accuracy: {val_acc:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v-U1PLlougCA"},"outputs":[],"source":["best_val_acc = 0\n","wait = 0\n","patience = 3  # stop after 3 epochs with no improvement\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        loss = F.cross_entropy(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    print(f\"\\nEpoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f}\")\n","\n","    # Validation\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in valid_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (preds == labels).sum().item()\n","    val_acc = 100 * correct / total\n","    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n","\n","    # Save best model\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        wait = 0\n","        torch.save(model.state_dict(), \"best_model.pt\")\n","        print(f\"‚úÖ Best model saved with {val_acc:.2f}% accuracy\")\n","    else:\n","        wait += 1\n","        if wait >= patience:\n","            print(\"‚èπÔ∏è Early stopping triggered.\")\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEjbKYubum3e"},"outputs":[],"source":["train_losses = []\n","val_accuracies = []\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yg2d-fdjup72"},"outputs":[],"source":["train_losses.append(total_loss)\n","val_accuracies.append(val_acc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3gMUF8IurGh"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses, label='Train Loss', color='blue')\n","plt.title('Training Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.grid()\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(val_accuracies, label='Validation Accuracy', color='green')\n","plt.title('Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy (%)')\n","plt.grid()\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig(\"training_curves.png\")  # Save the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrYyz4blxxoc"},"outputs":[],"source":["class HistologyCLIPDataset(Dataset):\n","    def __init__(self, root_dir, preprocess, classnames):\n","        self.root_dir = root_dir\n","        self.preprocess = preprocess\n","        valid_dataset.classnames\n","        self.classes = classnames     # ‚úÖ This is what classification_report needs\n","        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classnames)}\n","\n","        self.image_paths = []\n","        self.labels = []\n","\n","        for class_name in classnames:\n","            class_path = os.path.join(root_dir, class_name)\n","            for root, _, files in os.walk(class_path):\n","                for file in files:\n","                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n","                        self.image_paths.append(os.path.join(root, file))\n","                        self.labels.append(self.class_to_idx[class_name])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPjYG2OJ5HVN"},"outputs":[],"source":["class HistologyCLIPDataset(Dataset):\n","    def __init__(self, root_dir, preprocess, classnames):\n","        self.root_dir = root_dir\n","        self.preprocess = preprocess\n","        self.classnames = classnames  # ‚úÖ Provided list like [\"benign\", \"malignant\"]\n","        self.classes = classnames     # ‚úÖ This is what classification_report needs\n","        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classnames)}\n","\n","        self.image_paths = []\n","        self.labels = []\n","\n","        for class_name in classnames:\n","            class_path = os.path.join(root_dir, class_name)\n","            for root, _, files in os.walk(class_path):\n","                for file in files:\n","                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n","                        self.image_paths.append(os.path.join(root, file))\n","                        self.labels.append(self.class_to_idx[class_name])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfugr_TJ6-Yy"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from sklearn.metrics import classification_report, confusion_matrix\n","import numpy as np\n","\n","# Ensure model is in evaluation mode\n","model.eval()\n","\n","# Create validation loader\n","val_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n","\n","# Containers for predictions and labels\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for images, labels in val_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(images)\n","        _, preds = torch.max(outputs, 1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","# üìä Classification Report\n","print(\"Classification Report:\\n\", classification_report(all_labels, all_preds, target_names=valid_dataset.classnames))\n","\n","# üìâ Confusion Matrix\n","cm = confusion_matrix(all_labels, all_preds)\n","print(\"Confusion Matrix:\\n\", cm)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1p5uwjVpNp8qFZWd-sbCAQ6K1YcTot77i","authorship_tag":"ABX9TyPgflO3XavLqwd72Npo0fyi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02ff6904a77642e6ac8b16a556364dbf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08d17d39f5ed4c319482b6fe5f80d6bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15028bbd37ef45bcb2cf9e465ca96d83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f765a1cec627421cb43f5adc7395f3c3","max":605143316,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f562f9bc980f4bb8b56127082495eba8","value":605143316}},"5e6c59004aa9483e90298af17b88697f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b596e26bdcf470dbb39b488999763f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"901555769a7b45f9964a3322c709f462":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d331c4417395438fa6b7d26c711e1bd8","IPY_MODEL_15028bbd37ef45bcb2cf9e465ca96d83","IPY_MODEL_a6c372c358534887b4edbbc0c956711a"],"layout":"IPY_MODEL_c6c7a4f78deb4b4fae029b1aa9d0105a"}},"a6c372c358534887b4edbbc0c956711a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ff6904a77642e6ac8b16a556364dbf","placeholder":"‚Äã","style":"IPY_MODEL_08d17d39f5ed4c319482b6fe5f80d6bb","value":"‚Äá605M/605M‚Äá[00:04&lt;00:00,‚Äá101MB/s]"}},"c6c7a4f78deb4b4fae029b1aa9d0105a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d331c4417395438fa6b7d26c711e1bd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e6c59004aa9483e90298af17b88697f","placeholder":"‚Äã","style":"IPY_MODEL_7b596e26bdcf470dbb39b488999763f7","value":"open_clip_model.safetensors:‚Äá100%"}},"f562f9bc980f4bb8b56127082495eba8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f765a1cec627421cb43f5adc7395f3c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}